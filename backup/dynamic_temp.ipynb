{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "\n",
    "This function handles merging, encoding, and splitting the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess_data(file_path, target_column):\n",
    "    # Load the dataset\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    # Separate categorical and numerical features\n",
    "    categorical_features = df.select_dtypes(include=['object']).columns\n",
    "    numerical_features = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "    # Target and features\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "\n",
    "    # One-hot encode categorical features\n",
    "    X_encoded = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, X_encoded.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training and Evaluation\n",
    "\n",
    "This function trains, tunes, and evaluates multiple models dynamically. It returns the best model based on performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "import joblib\n",
    "\n",
    "def train_and_evaluate_models(models, X_train, X_test, y_train, y_test):\n",
    "    results = {}\n",
    "    best_models = {}\n",
    "\n",
    "    # Function to evaluate each model\n",
    "    def evaluate_model(model_name, model, param_distributions):\n",
    "        if param_distributions:\n",
    "            search = RandomizedSearchCV(model, param_distributions, n_iter=10, scoring='neg_mean_absolute_percentage_error', cv=5, n_jobs=-1, verbose=1, random_state=42)\n",
    "            search.fit(X_train, y_train)\n",
    "            best_model = search.best_estimator_\n",
    "            print(f\"Best Params for {model_name}: {search.best_params_}\")\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            best_model = model\n",
    "\n",
    "        y_train_pred = best_model.predict(X_train)\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "        metrics = {\n",
    "            'MSE_train': mean_squared_error(y_train, y_train_pred),\n",
    "            'MAE_train': mean_absolute_error(y_train, y_train_pred),\n",
    "            'R2_train': r2_score(y_train, y_train_pred),\n",
    "            'MAPE_train': mean_absolute_percentage_error(y_train, y_train_pred) * 100,\n",
    "            'MSE_test': mean_squared_error(y_test, y_test_pred),\n",
    "            'MAE_test': mean_absolute_error(y_test, y_test_pred),\n",
    "            'R2_test': r2_score(y_test, y_test_pred),\n",
    "            'MAPE_test': mean_absolute_percentage_error(y_test, y_test_pred) * 100\n",
    "        }\n",
    "\n",
    "        results[model_name] = metrics\n",
    "        return best_model\n",
    "\n",
    "    for model_name, (model, param_distributions) in models.items():\n",
    "        best_model = evaluate_model(model_name, model, param_distributions)\n",
    "        best_models[model_name] = best_model\n",
    "\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    print(results_df)\n",
    "\n",
    "    best_model_name = results_df['R2_test'].idxmax()\n",
    "    print(f\"The best model is: {best_model_name}\")\n",
    "\n",
    "    joblib.dump(best_models[best_model_name], f'{best_model_name.lower().replace(\" \", \"_\")}_model.pkl')\n",
    "    \n",
    "    return best_models[best_model_name], results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dynamic Model Deployment\n",
    "\n",
    "This function can be used to load and use the best model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model(model_file, encoded_columns_file, new_data):\n",
    "    # Load the model\n",
    "    model = joblib.load(model_file)\n",
    "    encoded_columns = joblib.load(encoded_columns_file)\n",
    "\n",
    "    # Ensure new data matches the encoded structure\n",
    "    new_data_encoded = pd.get_dummies(new_data)\n",
    "    for col in encoded_columns:\n",
    "        if col not in new_data_encoded.columns:\n",
    "            new_data_encoded[col] = 0\n",
    "    new_data_encoded = new_data_encoded[encoded_columns]\n",
    "\n",
    "    # Predict using the model\n",
    "    predictions = model.predict(new_data_encoded)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Execution Workflow\n",
    "\n",
    "This part brings everything together: from data merging, model training, evaluation, and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code to run everything\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Preprocess the data\n",
    "    X_train, X_test, y_train, y_test, encoded_columns = preprocess_data('modified_dataset/cleaned_car_data.xlsx', 'price')\n",
    "\n",
    "    # Step 2: Define models and parameter grids\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "    from xgboost import XGBRegressor\n",
    "    from scipy.stats import uniform\n",
    "\n",
    "    models = {\n",
    "        'Linear Regression': (LinearRegression(), {}),\n",
    "        'Ridge Regression': (Ridge(), {'alpha': [0.1, 1.0, 10.0]}),\n",
    "        'Lasso Regression': (Lasso(), {'alpha': [0.1, 1.0, 10.0]}),\n",
    "        'Decision Tree': (DecisionTreeRegressor(), {'max_depth': [3, 5, 10]}),\n",
    "        'Random Forest': (RandomForestRegressor(random_state=42), {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }),\n",
    "        'XGBoost': (XGBRegressor(objective='reg:squarederror', random_state=42), {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': uniform(0.5, 0.5),\n",
    "            'colsample_bytree': uniform(0.5, 0.5),\n",
    "            'alpha': [0, 0.1, 0.5, 1],\n",
    "            'lambda': [0, 0.1, 0.5, 1]\n",
    "        }),\n",
    "        'AdaBoost': (AdaBoostRegressor(random_state=42), {'n_estimators': [50, 100, 150]})\n",
    "    }\n",
    "\n",
    "    # Step 3: Train and evaluate models\n",
    "    best_model, results_df = train_and_evaluate_models(models, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Step 4: Save encoded columns for deployment\n",
    "    joblib.dump(encoded_columns, 'encoded_columns.pkl')\n",
    "\n",
    "    # Deployment example: Predict on new data\n",
    "    # Assume new_data is a DataFrame with similar structure\n",
    "    new_data = pd.DataFrame({\n",
    "        'make': ['Ford'],\n",
    "        'year': [2020],\n",
    "        'fuel': ['Diesel'],\n",
    "        'mileage': [25.0],\n",
    "        'transmission': ['Manual'],\n",
    "        'engine': [1500]\n",
    "    })\n",
    "\n",
    "    predictions = deploy_model('best_model.pkl', 'encoded_columns.pkl', new_data)\n",
    "    print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
